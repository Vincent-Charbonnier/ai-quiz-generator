{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ccf494c3-c8c7-4ed4-b868-9856e570f744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Deleted existing Chroma directory\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "CHROMA_DIR = \"/home/jovyan/chroma-e5-1024\"\n",
    "\n",
    "if os.path.exists(CHROMA_DIR):\n",
    "    shutil.rmtree(CHROMA_DIR)\n",
    "    print(\"ðŸ§¹ Deleted existing Chroma directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0beb3e5-02e9-40b3-99a4-5dc429b836fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== CONFIG ====\n",
    "PDF_PATH = \"./Certify4Sure.NCA-AIIO.pdf\"      # mounted PDF\n",
    "CHROMA_DIR = \"/home/jovyan/chroma-e5-1024\"              # persistent volume recommended\n",
    "\n",
    "EMBEDDING_ENDPOINT = \"https://nv-embedqa-e5-v5.vincent-charbon-8e171347.serving.pcaidev.ai.greendatacenter.com/v1\"\n",
    "EMBEDDING_TOKEN = \"eyJhbGciOiJSUzI1NiIsImtpZCI6ImxGN2t0SzQ1cllRUHFPcTh4UTZRbm9MT05mbTFvNkt4S29XTXcyb1BzYTgifQ.eyJhdWQiOlsiYXBpIiwiaXN0aW8tY2EiXSwiZXhwIjoxNzcyMDM0ODU2LCJpYXQiOjE3Njk0NDI4NTYsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiZTg4NDk3YjItNGE3MS00ODBkLTgyMWMtZDc5ZGNiNWQxYjI0Iiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJ1aSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJpc3ZjLWVwLTE3Njk0NDI4NTYwMTAiLCJ1aWQiOiI5YmZhYzk5ZS1iOGZmLTQ5YzMtOWViMS0zMDRmZjdhYWMyNWQifX0sIm5iZiI6MTc2OTQ0Mjg1Niwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OnVpOmlzdmMtZXAtMTc2OTQ0Mjg1NjAxMCJ9.HvhvN97ECyeY5nTFRf0MdZMJK6DN2coOB6KVAYE_q_wgOk9mLE-Hl2tBf5b6W1YMzBh-jy6YCUDPWER8NWBnVtcA2BFaoH7dZt4G_in50EWMUqxWS051jRfeMvD93KsbrGO-NeoPumDvjsRRllVumhEh8e3Ojt5ood_CroVjuETii4nxxfBDI_wtwD39jGaffWHM8CRAkkzkK_PKzVAWMw207v0TwYr1I-fZsRFJoR5_Kqk_chg10_wI2hbXhkCew85Oo8isRsXRTkslGMUFAEnqtBKkZree05wDBjbj7oGB0uiJoOuHJ-KMeOWIo5c6UH84K0b2-upwzBfMb9TW9g\"\n",
    "LLM_ENDPOINT = \"https://gpt-oss-120b.project-user-claudio-luethi.serving.pcaidev.ai.greendatacenter.com/v1\"\n",
    "LLM_TOKEN = \"eyJhbGciOiJSUzI1NiIsImtpZCI6ImxGN2t0SzQ1cllRUHFPcTh4UTZRbm9MT05mbTFvNkt4S29XTXcyb1BzYTgifQ.eyJhdWQiOlsiYXBpIiwiaXN0aW8tY2EiXSwiZXhwIjoxODAwNTM5MjAzLCJpYXQiOjE3NjkwMDMyMDMsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiMGY2NjA0OTUtYzVjZS00OGVlLTgzNDUtNTMwMTQ5NTczOTYyIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJ1aSIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJpc3ZjLWVwLTE3NjkwMDMyMDM0OTgiLCJ1aWQiOiIzZTQ4ZjZkOS0wMTk0LTQ0YjktOGQ4NS01ODA0ZDRmMzkxMTUifX0sIm5iZiI6MTc2OTAwMzIwMywic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OnVpOmlzdmMtZXAtMTc2OTAwMzIwMzQ5OCJ9.k80RYQu1D2BT7LUf2h0guj7f6lg81-SXAXpWGotI7lf8ViNYFAMn8hzP4RrTQDUW4xtnbLjRYk2xWGbv2oJ7QA5u-LebcHrDgnI1OUH0yY0CwnXgPhQQx9WPd-ASl2UGDTWcetI-qH123-xBXC9EqF1GuNleijagGO2C-Ch9GAwoVTKr0e3c9AUU-IWQrrnjDbfND2HDvbBPNrppGTH8E277qvgUc40vwkMShkc-1b9cUGSYVEEWqMXEln793DnspRLn6ixzM7FD4bX0QfuOqCE-Hr57YFFnq9T4AZJh4tq2qYHM7S7-TM3KBiW51T7nBOquDMR0h-e_yuNANjVQNA\"\n",
    "\n",
    "# Disable SSL verification\n",
    "http_client = httpx.Client(\n",
    "    verify=False,\n",
    "    timeout=60.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f3ab8543-cfa1-48e6-a1e3-3632cc66ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 92 pages\n",
      "Created 729 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=64\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Loaded {len(documents)} pages\")\n",
    "print(f\"Created {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "209f57fc-1b8b-4e25-a6af-49740e43690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"nvidia/nv-embedqa-e5-v5\", \n",
    "    api_key=EMBEDDING_TOKEN,\n",
    "    base_url=EMBEDDING_ENDPOINT,\n",
    "    http_client=http_client,\n",
    "    chunk_size=1,              \n",
    "    max_retries=0             \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba960b44-7337-4330-aa96-68330a716af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "{\"object\":\"error\",\"message\":\"The model expects an input_type from one of `passage` or `query` but none was provided.\",\"detail\":{},\"type\":\"invalid_request_error\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "resp = requests.post(\n",
    "    f\"{EMBEDDING_ENDPOINT}/embeddings\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {EMBEDDING_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    },\n",
    "    json={\n",
    "        \"model\": \"nvidia/nv-embedqa-e5-v5\",  # âœ… FIXED\n",
    "        \"input\": \"test sentence\"\n",
    "    },\n",
    "    verify=False\n",
    ")\n",
    "\n",
    "print(resp.status_code)\n",
    "print(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9238511c-77da-425d-a9bd-260fad7dec48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "HTTP Request: POST https://nv-embedqa-e5-v5.vincent-charbon-8e171347.serving.pcaidev.ai.greendatacenter.com/v1/embeddings \"HTTP/1.1 500 Internal Server Error\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Failed at chunk 0: Internal Server Error\n",
      "âœ… ChromaDB persisted to: /home/jovyan/chroma-e5-1024\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from pathlib import Path\n",
    "\n",
    "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"pdf-rag-e5-1024\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=CHROMA_DIR\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(chunks):\n",
    "    try:\n",
    "        vectorstore.add_documents([doc])  # ðŸ”‘ ONE document at a time\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed at chunk {i}: {e}\")\n",
    "        break\n",
    "\n",
    "vectorstore.persist()\n",
    "\n",
    "print(\"âœ… ChromaDB persisted to:\", CHROMA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03ca7568-a99c-4a23-b7cc-ebdfbf95569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "import requests\n",
    "import re\n",
    "\n",
    "class NIME5Embedding(Embeddings):\n",
    "    def __init__(self, endpoint: str, token: str, model: str, max_chars: int = 2000):\n",
    "        self.endpoint = endpoint.rstrip(\"/\")\n",
    "        self.token = token\n",
    "        self.model = model\n",
    "        self.max_chars = max_chars\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \"\", text)\n",
    "        text = text.strip()\n",
    "\n",
    "        if len(text) > self.max_chars:\n",
    "            text = text[:self.max_chars]\n",
    "\n",
    "        if not text:\n",
    "            raise ValueError(\"Empty text after cleaning\")\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _embed(self, text: str, input_type: str) -> list[float]:\n",
    "        text = self._clean_text(text)\n",
    "\n",
    "        resp = requests.post(\n",
    "            f\"{self.endpoint}/embeddings\",\n",
    "            headers={\n",
    "                \"Authorization\": f\"Bearer {self.token}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "            },\n",
    "            json={\n",
    "                \"model\": self.model,\n",
    "                \"input\": text,\n",
    "                \"input_type\": input_type,  # ðŸ”‘ REQUIRED\n",
    "            },\n",
    "            verify=False,\n",
    "            timeout=60,\n",
    "        )\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(f\"NIM error {resp.status_code}: {resp.text}\")\n",
    "\n",
    "        return resp.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    # Used by Chroma during ingestion\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        vectors = []\n",
    "        for t in texts:\n",
    "            try:\n",
    "                vectors.append(self._embed(t, input_type=\"passage\"))\n",
    "            except Exception as e:\n",
    "                print(\"âš ï¸ Skipping chunk:\", e)\n",
    "        return vectors\n",
    "\n",
    "    # Used at query time\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self._embed(text, input_type=\"query\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94ee6d61-d7c2-48a0-9c3d-79b1992a892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c197e95-87b4-4d4e-b657-fa81b85948f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 0/729 chunks\n",
      "Ingested 25/729 chunks\n",
      "Ingested 50/729 chunks\n",
      "Ingested 75/729 chunks\n",
      "Ingested 100/729 chunks\n",
      "Ingested 125/729 chunks\n",
      "Ingested 150/729 chunks\n",
      "Ingested 175/729 chunks\n",
      "Ingested 200/729 chunks\n",
      "Ingested 225/729 chunks\n",
      "Ingested 250/729 chunks\n",
      "Ingested 275/729 chunks\n",
      "Ingested 300/729 chunks\n",
      "Ingested 325/729 chunks\n",
      "Ingested 350/729 chunks\n",
      "Ingested 375/729 chunks\n",
      "Ingested 400/729 chunks\n",
      "Ingested 425/729 chunks\n",
      "Ingested 450/729 chunks\n",
      "Ingested 475/729 chunks\n",
      "Ingested 500/729 chunks\n",
      "Ingested 525/729 chunks\n",
      "Ingested 550/729 chunks\n",
      "Ingested 575/729 chunks\n",
      "Ingested 600/729 chunks\n",
      "Ingested 625/729 chunks\n",
      "Ingested 650/729 chunks\n",
      "Ingested 675/729 chunks\n",
      "Ingested 700/729 chunks\n",
      "Ingested 725/729 chunks\n",
      "âœ… ChromaDB ingestion complete\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from pathlib import Path\n",
    "\n",
    "embedding_fn = NIME5Embedding(\n",
    "    endpoint=EMBEDDING_ENDPOINT,\n",
    "    token=EMBEDDING_TOKEN,\n",
    "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
    ")\n",
    "\n",
    "CHROMA_DIR = \"/home/jovyan/chroma-e5-1024\"\n",
    "Path(CHROMA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"pdf-rag\",\n",
    "    embedding_function=embedding_fn,\n",
    "    persist_directory=CHROMA_DIR,\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(chunks):\n",
    "    vectorstore.add_documents([doc])\n",
    "    if i % 25 == 0:\n",
    "        print(f\"Ingested {i}/{len(chunks)} chunks\")\n",
    "\n",
    "print(\"âœ… ChromaDB ingestion complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69c82009-064b-4592-8530-f9a56ee1ba36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='5e7fac29-c145-4285-86a0-56d756967076', metadata={'producer': 'Foxit Reader PDF Printer Version 9.3.0.1233', 'creator': '', 'source': './Certify4Sure.NCA-AIIO.pdf', 'keywords': '', 'page': 19, 'total_pages': 92, 'subject': '', 'moddate': '2025-06-08T17:06:46-07:00', 'creationdate': '2025-06-08T17:06:46-07:00', 'author': '', 'title': '', 'page_label': '20'}, page_content='ensuring comprehensive understanding and accuracy. Typographical errors in the original questions have been'),\n",
       " Document(id='a7f37a34-5aaf-4771-8133-f0728b041add', metadata={'creationdate': '2025-06-08T17:06:46-07:00', 'author': '', 'moddate': '2025-06-08T17:06:46-07:00', 'source': './Certify4Sure.NCA-AIIO.pdf', 'page': 0, 'subject': '', 'title': '', 'page_label': '1', 'creator': '', 'producer': 'Foxit Reader PDF Printer Version 9.3.0.1233', 'total_pages': 92, 'keywords': ''}, page_content='Nvida NCA-AIIO : Practice Test\\n    \\nExam Code: NCA-AIIO\\nTitle : AI Infrastructure and Operations'),\n",
       " Document(id='bbfa5774-71dc-4a1c-8de9-f407ec44becf', metadata={'page_label': '76', 'subject': '', 'creationdate': '2025-06-08T17:06:46-07:00', 'creator': '', 'producer': 'Foxit Reader PDF Printer Version 9.3.0.1233', 'moddate': '2025-06-08T17:06:46-07:00', 'title': '', 'page': 75, 'source': './Certify4Sure.NCA-AIIO.pdf', 'total_pages': 92, 'keywords': '', 'author': ''}, page_content='NVIDIA\\'s \"AI Infrastructure and Operations Fundamentals\" course and \"NVIDIA DCGM\" documentation\\nencourage such visualizations for performance analysis, as they provide actionable insights into resource\\nimpacts on training efficiency. A bar chart (A) shows averages but obscures session-specific relationships. A\\nhistogram (B) displays distribution, not pairwise relationships. A line chart (C) implies temporal continuity, which')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search(\"What is this document about?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fbea945-6f53-465c-a589-9273f0976ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exam A\n",
      "QUESTION 1\n",
      "Which NVIDIA solution is specifically designed to accelerate data analytics and machine learning workloads,\n",
      "allowing data scientists to build and deploy models at scale using GPUs?\n",
      "A.\n",
      "NVIDIA CUDA\n",
      "B.\n",
      "NVIDIA JetPack\n",
      "C.\n",
      "NVIDIA RAPIDS\n",
      "D.\n",
      "NVIDIA DGX A100\n",
      "Correct Answer: \n",
      "C\n",
      "Explanation\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "docs = vectorstore.similarity_search(\n",
    "    \"Select 2 questions from this test with options A, B, C etc.\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(docs[0].page_content[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0539499c-a55c-487d-b156-2c061327ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_chunks(vectorstore, n_chunks: int = 20):\n",
    "    \"\"\"\n",
    "    Fetch random chunks from Chroma.\n",
    "    We over-sample because not every chunk is a full question.\n",
    "    \"\"\"\n",
    "    collection = vectorstore._collection  # Chroma internal handle\n",
    "\n",
    "    count = collection.count()\n",
    "    if count == 0:\n",
    "        raise ValueError(\"Vector store is empty\")\n",
    "\n",
    "    # Random indices\n",
    "    indices = random.sample(range(count), min(n_chunks, count))\n",
    "\n",
    "    results = collection.get(\n",
    "        include=[\"documents\"],\n",
    "        ids=[collection.get()[\"ids\"][i] for i in indices]\n",
    "    )\n",
    "\n",
    "    return results[\"documents\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9399c120-aefd-448b-b4b9-a548ec4b5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCQ_SELECTION_PROMPT = \"\"\"\n",
    "You are given excerpts from a document that contains multiple-choice questions.\n",
    "\n",
    "Each question has:\n",
    "- A question text\n",
    "- Four options labeled A, B, C, D\n",
    "\n",
    "Your task:\n",
    "- Select exactly 5 COMPLETE and DISTINCT questions\n",
    "- Each question must include all four answer options (Aâ€“D)\n",
    "- Do NOT invent new questions\n",
    "- Do NOT modify wording\n",
    "- If fewer than 5 valid questions exist, return as many as possible\n",
    "\n",
    "Return the result in this format:\n",
    "\n",
    "Question 1:\n",
    "<question text>\n",
    "A. ...\n",
    "B. ...\n",
    "C. ...\n",
    "D. ...\n",
    "\n",
    "Question 2:\n",
    "...\n",
    "\n",
    "Here are the document excerpts:\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb2eefdb-71ae-4995-8d15-65a8d1ef1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-oss-120b\",      # adjust if needed\n",
    "    api_key=LLM_TOKEN,\n",
    "    base_url=LLM_ENDPOINT,\n",
    "    http_client=http_client,\n",
    "    temperature=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2e6cb4f-0223-4cde-a1ff-e3173f68d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_mcqs(vectorstore, llm, num_questions: int = 5):\n",
    "    # 1. Sample chunks\n",
    "    docs = get_random_chunks(vectorstore, n_chunks=30)\n",
    "\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "\n",
    "    # 2. Build prompt\n",
    "    prompt = MCQ_SELECTION_PROMPT.format(context=context)\n",
    "\n",
    "    # 3. Ask model\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c0ee93d4-9075-44a6-9ad0-19877c4425a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://gpt-oss-120b.project-user-claudio-luethi.serving.pcaidev.ai.greendatacenter.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Question 1:**  \n",
      "Your team is running an AI inference workload on a Kubernetes cluster with multiple NVIDIA GPUs. You observe that some nodes with GPUs are underutilized, while others are overloaded, leading to inconsistent inference performance across the cluster. Which strategy would most effectively balance the GPU workload across the Kubernetes cluster?  \n",
      "A. Triton's role in production inference is wellâ€‘documented in NVIDIA's AI ecosystem (A).  \n",
      "B. NVIDIA TensorRT optimizes models for highâ€‘performance inference but is a library for model optimization, not a deployment server.  \n",
      "C. NVIDIA NGC Catalog is a repository of GPUâ€‘optimized containers and models, useful for sourcing but not managing deployment.  \n",
      "D. NVIDIA CUDA Toolkit is a development platform for GPU programming, not a deployment solution.  \n",
      "\n",
      "---\n",
      "\n",
      "**Question 2:**  \n",
      "In your AI data center, you need to ensure continuous performance and reliability across all operations. Occasionally, some highâ€‘priority jobs experience delays because lowerâ€‘priority jobs are consuming GPU resources. Which of the following actions would most effectively ensure that highâ€‘priority jobs are allocated GPU resources first?  \n",
      "A. Increase the number of GPUs in the cluster  \n",
      "B. Configure Kubernetes pod priority and preemption  \n",
      "C. Manually assign GPUs to highâ€‘priority jobs  \n",
      "D. Use Kubernetes node affinity to bind jobs to specific nodes  \n",
      "\n",
      "---\n",
      "\n",
      "**Question 3:**  \n",
      "You are working with a team of data scientists on an AI project where multiple machine learning models are being trained to predict customer churn. The models are evaluated based on the Mean Squared Error (MSE). Server with Dynamic Batching groups incoming requests into batches dynamically, smoothing out processing and reducing spikes on NVIDIA GPUs in a Kubernetes cluster (e.g., DGX). This ensures low latency, critical for user interaction. MIG (Option A) isolates workloads but doesn't address batching. More replicas (Option C) scale throughput, not latency consistency. Quantization (Option D) speeds inference but may not eliminate spikes. Triton's dynamic batching is NVIDIA's solution for this. Compared to running workloads on bare metal, which factor is most likely contributing to the performance degradation?  \n",
      "A. Using highâ€‘performance networking.  \n",
      "B. Overcommitting GPU resources.  \n",
      "C. Running VMs on SSD storage.  \n",
      "D. Enabling high availability features.  \n",
      "\n",
      "---\n",
      "\n",
      "**Question 4:**  \n",
      "When deploying AI workloads on a cloud platform using NVIDIA GPUs, which of the following is the most critical consideration to ensure cost efficiency without compromising performance?  \n",
      "A. Running all workloads on a single, highâ€‘performance GPU instance to minimize costs  \n",
      "B. Using spot instances where applicable for nonâ€‘critical workloads  \n",
      "C. Choosing a cloud provider that offers the lowest perâ€‘hour GPU cost  \n",
      "D. Selecting the instance with the maximum GPU memory available  \n",
      "\n",
      "---\n",
      "\n",
      "**Question 5:**  \n",
      "Which component of the NVIDIA AI software stack is primarily responsible for optimizing deep learning inference performance by leveraging the specific architecture of NVIDIA GPUs?  \n",
      "A. NVIDIA cuDNN  \n",
      "B. NVIDIA TensorRT  \n",
      "C. NVIDIA Triton Inference Server  \n",
      "D. NVIDIA CUDA Toolkit\n"
     ]
    }
   ],
   "source": [
    "questions = generate_random_mcqs(vectorstore, llm, num_questions=5)\n",
    "print(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d07d294-a89a-4809-a845-ba4a4f511d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "STRICT_JSON_PROMPT = \"\"\"\n",
    "You are a quiz generator.\n",
    "\n",
    "You MUST return ONLY a valid JSON array.\n",
    "DO NOT include markdown, explanations, or extra text.\n",
    "DO NOT wrap the JSON in backticks.\n",
    "DO NOT include newlines before or after the JSON.\n",
    "\n",
    "Each element MUST have exactly this structure:\n",
    "{\n",
    "  \"id\": number,\n",
    "  \"question\": string,\n",
    "  \"options\": [string, string, string, string],\n",
    "  \"correctIndex\": number\n",
    "}\n",
    "\n",
    "Rules:\n",
    "- Generate exactly {n} questions.\n",
    "- Each question must have exactly 4 options.\n",
    "- correctIndex must be 0, 1, 2, or 3.\n",
    "- Questions MUST come from the provided context.\n",
    "- Do NOT invent new questions.\n",
    "- IDs must be sequential starting at 1.\n",
    "\n",
    "Context:\n",
    "----------------\n",
    "{context}\n",
    "----------------\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a2e93239-bba1-4c92-82f8-147b61e4f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def generate_mcq_json(vectorstore, llm, n_questions=5):\n",
    "    # 1. Random sampling (oversample)\n",
    "    docs = get_random_chunks(vectorstore, n_chunks=40)\n",
    "    context = \"\\n\\n\".join(docs)\n",
    "\n",
    "    # 2. Prompt\n",
    "    prompt = STRICT_JSON_PROMPT.format(\n",
    "        n=n_questions,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # 3. LLM call\n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "\n",
    "    # 4. Extract first JSON array defensively\n",
    "    match = re.search(r\"\\[[\\s\\S]*\\]\", content)\n",
    "    if not match:\n",
    "        raise ValueError(\"Model did not return a JSON array\")\n",
    "\n",
    "    json_text = match.group(0)\n",
    "\n",
    "    # 5. Validate JSON\n",
    "    data = json.loads(json_text)\n",
    "\n",
    "    # 6. Final validation (important)\n",
    "    if len(data) != n_questions:\n",
    "        raise ValueError(\"Wrong number of questions\")\n",
    "\n",
    "    for q in data:\n",
    "        assert set(q.keys()) == {\"id\", \"question\", \"options\", \"correctIndex\"}\n",
    "        assert len(q[\"options\"]) == 4\n",
    "        assert 0 <= q[\"correctIndex\"] <= 3\n",
    "\n",
    "    return json_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "89623e4d-ce29-4af5-b59a-1b7cf3c611e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, HTTPException\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat/completions\")\n",
    "async def chat_completions(req: Request):\n",
    "    body = await req.json()\n",
    "\n",
    "    try:\n",
    "        # Extract desired N from user prompt if needed\n",
    "        user_msg = next(m[\"content\"] for m in body[\"messages\"] if m[\"role\"] == \"user\")\n",
    "\n",
    "        # Default to 5 if not specified\n",
    "        n = int(re.search(r\"\\b(\\d+)\\b\", user_msg).group(1)) if re.search(r\"\\b(\\d+)\\b\", user_msg) else 5\n",
    "\n",
    "        json_array = generate_mcq_json(\n",
    "            vectorstore=vectorstore,\n",
    "            llm=llm,\n",
    "            n_questions=n\n",
    "        )\n",
    "\n",
    "        return openai_chat_response(json_array)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fb88227-cf05-4e0c-90a6-21a370f8b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from fastapi import FastAPI, Request, HTTPException\n",
    "import re\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat/completions\")\n",
    "async def chat_completions(req: Request):\n",
    "    body = await req.json()\n",
    "\n",
    "    try:\n",
    "        user_msg = next(m[\"content\"] for m in body[\"messages\"] if m[\"role\"] == \"user\")\n",
    "\n",
    "        # Extract N, default to 5\n",
    "        match = re.search(r\"\\b(\\d+)\\b\", user_msg)\n",
    "        n = int(match.group(1)) if match else 5\n",
    "\n",
    "        json_array = generate_mcq_json(\n",
    "            vectorstore=vectorstore,\n",
    "            llm=llm,\n",
    "            n_questions=n\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"content\": json_array\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2fbc3-17c1-432b-a533-66e427633cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2696]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "\n",
    "uvicorn.run(\n",
    "    app,\n",
    "    host=\"0.0.0.0\",\n",
    "    port=8000,\n",
    "    log_level=\"info\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15356c4b-f9f9-4f82-aafc-cbff8d9dd82b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
